% template!!
\documentclass[sigconf,anonymous]{acmart}

\usepackage{amsmath}
\usepackage[linesnumbered,boxed]{algorithm2e}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{tikz}
\usetikzlibrary{bayesnet}

\newenvironment{shrinkeq}[1]
{ \bgroup
  \addtolength\abovedisplayshortskip{#1}
  \addtolength\abovedisplayskip{#1}
  \addtolength\belowdisplayshortskip{#1}
  \addtolength\belowdisplayskip{#1}}
{\egroup\ignorespacesafterend}



\setcopyright{none}

\begin{document}

%???MNAR title?
\title{Drug Target Interaction Prediction with Missing not at Random Labels}

%\author{Chen Lin, Sheng Ni, Xiangxiang Zeng}
\begin{abstract}
We introduce a novel probabilistic model, \textbf{F}actorization with \textbf{M}issing \textbf{N}ot at \textbf{R}andom \textbf{L}abels (FMNRL), for \textbf{D}rug-\textbf{T}arget \textbf{I}nteraction (DTI) prediction. Unlike previous studies which label unknown DTIs as negative samples, we treat the unnown DTIs as missing not at random responses. FMNRL assumes the labels are probabilistically generated from feature vectors of both drugs and targets, and a hidden matrix mapping from the drug features to target features. By associating the possibility of a missing response to the possibility of a negative label, FMNRL can learn the hidden feature space mapping better and thus provide more accurate DTI predictions. Experimental results show that FMNRL outperforms state-of-the-art methods.

\end{abstract}


\keywords{Missing Not At Random, Drug Target Interaction Prediction, Probabilistic Factor Models}

\maketitle
\section{Introduction}\label{sec:introduction}
%Background: DTI
\textbf{D}rug-\textbf{T}arget \textbf{I}nteraction (DTI) is fundamental to drug discovery and design. As biochemical experimental methods for DTI identification are extremely costly and time-consuming, computational DTI prediction methods have received a growing popularity in literature. The mojority of existing computational methods treat DTI prediction as a binary classification task, where known DTIs are labeled as positive and unknown DTIs are labeled as negative~\cite{Ding2013Similarity}. To address the imbalanced problem arised from the binary classification scheme, many research works attempt to extract a subset of reliable negative samples, e.g. by random sampling~\cite{Luo2017Network} or by \textbf{U}nlabel \textbf{L}earning (PU Learning)~\cite{Peng2017Screening}.

%MNAR intuition
Instead of labeling the unknown DTIs as negative, we argue that it is more natural to consider the unknown DTIs, i.e. DTIS that are neither identified in vivo to be positive nor experimentally validated to be negative (non-interacting drug-target pairs), as missing responses (i.e. the labels are missing). Our assumption in this work is that labels are not missing at random. This is an intuitive and reasonable assumption because researchers will use their domain expertise to filter DTIs with a high possiblity to be positive and priorize validations for these DTIs in vivo. For example, if researchers prefer to validate drugs with certain pharmacokinetic interactions, i.e. one drug affects the in vivo absorption, distribution, metabolism, or excretion of the target, then the unknown DTIs are not missing at random because drugs without pharmacokinetic interactions with the target are less likely to be positive and hence more likely to be missing.

\subsection{Contribution}
%present work
Our chief contribution in this work is a novel \textbf{F}actorization with \textbf{M}issing \textbf{N}ot at \textbf{R}andom \textbf{L}abels model (FMNRL). To the best of our knowledge, this is the first time missing not at random theory is applied in DTI identification. The inputs of FMNRL are feature vectors of drugs and targets which are leant and integrated from heterogenous data sources, the partially observed labels (i.e. positive or negative), and the fully observed reponses (i.e. given or missing). The FMNRL model mimics the probabilitic procedures to generate the labels from feature vectors and the responses from labels. Specifically, the labels are related to feature vectors of both drugs and targets, and a hidden matrix mapping from the drug features to target features. The possibility of giving a response is associated to the possibility of a positive label.

We conduct experiments on a large-scale DPI database. Our model achieves significantly better AUPR result and comparable AUROC result to the best of related works~\cite{Luo2017Network}. In the biomedical field, AUROC is considered to be a more robust and better assessment than AUROC~\cite{Luo2017Network}. Thus our improvment in AUPR is promising.
\subsection{Related Work}
%distinguision
One component of our work (i.e labels are generated by feature vectors learnt and fused from heterogenous information networks) is inspired by a recent work DTINet~\cite{Luo2017Network}. However, there are three key differences between our work and DTINet. (1) DTINet is based on deteministic matrix factorization, our work is based on probabilistic factor models. For example, the hidden feature space mapping matrix, labels, and responses are all random variables. This setting enables the FMNRL model to regulate the parameters (i.e. hidden feature space mapping matrix) by introducing approapriate priors and improves performance on sparse data set. (2) DTINet is based on randomly missing responses, i.e. it samples uniformly a set of unknown DTIs as negative sample, while FMNRL is based on missing not at random theories. Statistical theory in~\cite{Little1987Statistical} shows that applying a model based on missing at random assumptions can lead to biased parameter estimation on data sets with missing not at random entries. (3) DTINet adopts only a subset of unknown DTIs to preserve a balanced number of positive and negative samples, while our model uses all information in the data set.

We also want to distinguish our work with another line of research. Usually only positive DTIs are deposited in known databases. Due to the lack of negative samples, recently \textbf{P}ositive \textbf{U}nlabel \textbf{L}earning (PU Learning) is empolyed in DTI identification, e.g. to facilitate negative sample extraction~\cite{Peng2017Screening}. PU learning does not explicitly associate the status of an instance (i.e. being positive or unlabel) with the value of its hidden label. We also want to mention here that, although we experiment with datasets where only positive DTIs are deposited, FMNRL is extendable without difficulty to databases where positive and negative DTIs are available. Thus our model is applicable in more scenarios.


%structure
%This paper is structured as follows.

\section{The Proposed Method}\label{sec:method}
We start with the problem definitions and notations in Sec.~\ref{sec:input}. We then describe the proposed model FMNRL in Sec.~\ref{sec:model}. Finally we present the influence algorithm in Sec.~\ref{sec:inference}.

\subsection{Preliminaries}\label{sec:input}
DTI identification is often modeled as a binary classification task. Formally, we are given $P\in \mathcal{R}^{N\times M}$ a set of DTI labels, where $p_{i,j}=0$ indicates a negative interaction between drug $i$ and target $j$, $p_{i,j}=1$ indicates a positive DTI, the feature vectors on drug side $X\in \mathcal{R}^{N \times K}$, where $x_{i,k}$ represents drug $i$'s weight on drug feature $k$, the feature vectors on target side $Y\in \mathcal{R}^{M \times L}$, where $y_{j,l}$ represents target $j$'s weight on target feature $l$. The problem is to predict for a new drug-target pair $<i',j'>$, the possibility of a positive DTI $p(p_{i',j'}=1)$.

It is worthy to note that many previous works have shown that extracting features $X,Y$ from heterogenous data sources are beneficial. Similar to DTINet~\cite{Luo2017Network}, we use a compact feature expression learnt from aggregating diffusion probabilities on multiple information networks. For example, on the drug side, we can collect drug side-effect network, drug similarity network and so on. On the target side, we can collect protein desease network, protein protein network and so on. The feature learning phase is beyond the scope of this paper. We refer the readers to~\cite{Luo2017Network}.

In addition to the features $X,Y$ and labels $P$, we make one essential modification to the problem definition. We assume that the inputs also contain responses $R\in \mathcal{R}^{N\times M}$, where $R_{i,j}=0$ indicates that an unknown DTI, $R_{i,j}=1$ indicates a verified DTI (positive or negative). For positive responses $R_{i,j}=1$, the labels $P_{i,j}$ are observed. For negative responses $R_{i,j}=1$, the labels are hidden and unknown.

\subsection{Model}\label{sec:model}

We use a factor model, depicted in Fig.~\ref{fig:model}. The features $X,Y$ are in different dimensions. To associate the drug features with the target features, we introduce a hidden variable $Z\in\mathcal{R}^{K\times L}$, where $Z_{k,l}$ is a projection that maps the drug feature dimension $k$ to the target feature dimension $l$. We assume that $Z$ is sampled from a Gaussian distribution,

\begin{equation}\label{equ:z}
\forall k,l, Z_{k,l}\sim \mathcal{N}(0,\sigma^2),
\end{equation}
where $\sigma^2$ is the variance. We use zero mean to favor sparse feature mapping, i.e. a drug feature $k$ is associated with a few target features.

\begin{figure}\label{fig:model}
  \centering
  \tikz{ %
%hyper parameters
  %latent nodes
    \node[const] (x) {$X$} ; %
    \node[const, right =of x] (y) {$Y$};
     \node[const, right =of y] (sigma) {$\sigma^2$};
    \node[latent, below = of sigma] (z) {$Z$};
       %per interaction
    \node[obs, below=2 of z](p){$P$};
      \node[obs, below= of p] (r){$R$};
  \node[latent, right = 2 of r] (beta){$\beta$};
  \node[const,right = of beta](eta) {$\eta$};
    \edge{x}{p};
    \edge{sigma}{z};
    \edge{y}{p};
    \edge{z}{p};
    \edge{p}{r};
  \edge{beta}{r};
  \edge{eta}{beta};
     \plate[inner sep=0.2cm, xshift=-0.12cm, yshift=0.12 cm] {plate} {(p) (r)} {N};
   \plate[inner sep=0.2cm, xshift=-0.12cm, yshift=0.12 cm] {plate2} {(beta)} {2};

 }
\caption{Graphical Representation of the FMNRL model}
\end{figure}

We then assume that the binary label $P_{i,j}$ is generated from the following process.
\begin{equation}\label{equ:p}
\forall i,j, p(P_{i,j}=1|X,Y,Z)=\frac{1}{1+\exp{(XZY)}_{i,j}}
\end{equation}

The binary response is sampled from a Bernouli distribution. The parameters of the Bernouli distribution are related to the value of each $P_{i,j}$. Therefore we define $\beta_p\in\mathcal{R}^{2},p\in \{0,1\}$, $\forall p, \beta_{p,0}>0,\beta_{p,1}>0,\beta_{p,0}+\beta_{p,1}=1$, we have:
\begin{eqnarray}
\forall p\in \{0,1\}, \beta_p \sim Beta(\eta),\\
\forall i,j, R_{i,j} \sim Bern (\beta_{P_{i,j},1}),
\end{eqnarray}
where $\eta$ is the hyperparameter for the Beta distribution.

\subsection{Inference}\label{sec:inference}
The objective is to maximize the likelihood which consists of two terms. The first term is on partial observations, i.e. $R_{i,j}=0$ and $P_{i,j}$ unknown. The second term is on full observations, i.e. $R_{i,j}=1$ and known $P_{i,j}$.
\begin{equation}\label{equ:loss}
\mathcal{L}=\sum_{R_{i,j}=0}\sum_{P_{i,j}} \log p(R_{i,j},P_{i,j}|X,Y,\sigma^2,\eta) + \sum_{R_{i,j}=1} \log p(R_{i,j},P_{i,j}|X,Y,\sigma^2,\eta)
\end{equation}

Direct optimization for both terms in Equ.~\ref{equ:loss} are intractable, as they involve integration over continuous hidden variables. For example, $p(R|X,Y,\sigma^2,\eta)=\int_{P,Z,\beta} p(R|P,\beta,\eta) p(P|X,Y,Z) p(Z|\sigma^2) $. We employ the variational inference algorithm to infer the parameters. That is, we use the mean field assumption to factorize the posterior distribution: 

\begin{equation}
    q(P,Z,\beta|R,X,Y,\sigma^{2},\eta) = q(P|\theta)q(Z|\mu,\upsilon)q(\beta|\rho),
\end{equation}

It is convenient if $q(P|\theta),q(Z|\mu,\upsilon),q(\beta|\rho)$ are exponential distributions. We approximate the sigmoid function in Equ.~\ref{equ:p} by an exponential distribution. We use the property that any sigmoid function $\sigma(\cdot)$ has a lower bound:
\begin{equation}
Q(P|\theta)=\sigma(\theta)\geq \sigma(\zeta)\exp{(\theta-\zeta)/2-\lambda(\zeta)(\theta^2-\zeta^2)},
\end{equation}
where $\lambda(\zeta)=[\sigma(\zeta)-1/2]/[2\zeta]$.

As shown in Alg.~\ref{alg:model}, in each iteration of the inference we alternatively optimize the variational parameters for $q(Z),q(\beta), q(P)$ and the parameters for the lowerbound $\sigma(\cdot)$. We divide the data objects into two disjoint sets, $s_1 = \{(i,j)\in \mathcal{R}^{N\times M}|R_{i,j}=1\}$, and $s_2= \{(i,j)\in \mathcal{R}^{N\times M}|R_{i,j}=0\}$. In each iteration, we first obtain the optimal $\theta,\mu,v,\rho$ and then we update $\zeta$. The iteration is repeated until convergence is achieved.



\begin{algorithm}
    \SetKwInOut{Input}{input}
    \SetKwInOut{Output}{output}
    \Input{P, R, X, Y}
    \Output{$\mu$, $\upsilon$, $\rho$, $\theta$, $\zeta$}
    initialization\;
    \Repeat{convergence}{
        \For{$Z_{k,l} \in Z$}{
            $\mu_{k,l} \leftarrow \frac{\sum_{(i,j)\in s_2}(\theta_{i,j}-\frac{1}{2})X_{i,k}*Y_{j,l}+\sum_{(i,j)\in s_1}\frac{1}{2}X_{i,k}*Y_{j,l}}{2*(\sum_{i,j}\lambda(\zeta_{i,j})X^{2}_{i,k}Y_{j,l}^{2}+\frac{1}{\sigma^2})}$\;
            $\upsilon_{k,l} \leftarrow \frac{1}{\sqrt{2*(\sum_{i,j}\lambda(\zeta_{i,j})X^{2}_{i,k}Y_{j,l}^{2}+\frac{1}{\sigma^2})}}$\;
        }
        \For{$\beta$}{
            $\rho_{0,0} \leftarrow \eta_{0,0}$\;
            $\rho_{0,1} \leftarrow \sum_{(i,j)\in s_2}(1-\theta_{i,j})+\eta_{0,1}$\;
            $\rho_{1,0} \leftarrow \left|s_1\right| +\eta_{1,0}$\;
            $\rho_{1,1} \leftarrow \sum_{(i,j)\in s_2}\theta_{i,j}+\eta_{1,1}$\;
        }
        \For{$(i,j) \in s_2$}{
            $l_1 = exp(\psi(\rho_{1,1})-\psi(\rho_{1,0}+\rho_{1,1})+X_{i}\mu Y_{j}^{T})$\;
            $l_2 = exp(\psi(\rho_{0,1})-\psi(\rho_{0,0}+\rho_{0,1}))$\;
            $\theta_{i,j} \leftarrow \frac{l_1}{l_1+l_2}$\;
        }
        \For{$(i,j) \in s_1+s_2$}{
            $\zeta_{i,j} \leftarrow \left|X_{i}\mu Y_{j}^T\right|$\;
        }
    }
    \caption{FMNRL algorithms}\label{alg:model}
\end{algorithm}

\section{Experiment}\label{sec:experiment}
\subsection{Experimental Setup}

\textbf{Datasets.} %Include descriptions and statistics of the data sets here. highlight why we use this data set.
We use the same datasets as in~\cite{Luo2017Network}: i.e. the drug-target interaction labels are obtained from the latest version of DrugBank (version 3.0); the feature vectors $X$ are extracted three heterogenous networks: the drug-drug interaction network, the drug-disease network, drug-side-effect network and the drug structure similarity network; the feature vectors $Y$ are extracted from three heterogenous networks, the protein-disease association network, protein-protein network and protein sequence similarity network. As shown in Tab.~\ref{tab:data}, only $0.18\%$ of the drug-target interactions are labelled as positive, none is labelled as negative.

\begin{table}[htp]
\caption{Statistics of the dataset}\label{tab:data}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Data & \#Drugs & \#Targets & \#Positive & \#Negative & \#Unknown \\\hline
Count & 708 & 1512 & 1923 & 0 & 1068573 \\\hline
\end{tabular}
\end{center}
\label{default}
\end{table}%


\textbf{Comparative Methods.} We compare our FMNRL model with 5 state-of-the-art methods: (1) (2) (3) (4) (5)DTINet~\cite{Luo2017Network}, which is a regression model that learns feature space mapping $Z$ by the loss function  $\min_{Z} \sum_{i,j}(P_{i,j}-(XZY)_{i,j})^2$. We do not change the default settings for all the above comparative methods.

%\textbf{Preprocessing.} Include the networks we use

\textbf{Evaluation.} The major evaluation metric is  (AUPR), which is commonly adopted in bioinformatic studies. AUPR computes the area under precision recall curve. An auxiliary evaluation metric is (AUROC), which computes the area under receiver operating characteristic curve. We perform two cross-validation tests. The first one is on the full data set, i.e. we randomly segment the whole data set to 10 divisions and conduct 10-fold cross-validation. The second one is on the sampled data set.  We randomly sample the unknown drug-target interactions and construct a data set in which the ratio of positive and unknown samples is $1:1$ as in~\cite{Luo2017Network}.

\subsection{Results and Analysis}
\textbf{DTI Prediction Performance.}

\textbf{Stability.}

\textbf{Parameter Tunning.} If there's more space, include the effects of parameters $\sigma^2,\eta$.

\section{Conclusion}\label{sec:conclusion}


\begin{thebibliography}{10}
\bibitem{Ding2013Similarity}
Ding, H., Takigawa, I., Mamitsuka, H., and Zhu, S.
\newblock Similarity-based machine learning methods for predicting drug–target interactions: a brief review. \newblock In {\em Briefings in bioinformatics}, 15(5), 734-747.
\bibitem{Peng2017Screening}
Peng L, Zhu W, Liao B, et al.
\newblock Screening drug-target interactions with positive-unlabeled learning.
\newblock In {\em Scientific Reports}, 2017, 7(1): 8087.

\bibitem{Little1987Statistical}
R. J. A. Little and D. B. Rubin.
\newblock Statistical Analysis with Missing Data, 1987.

\bibitem{Luo2017Network}
Luo Y, Zhao X, Zhou J, et al.
\newblock A network integration approach for drug-target interaction prediction and computational drug repositioning from heterogeneous information.
\newblock In {\em Nature Communications}, 2017, 8(1).

%\bibitem{DrugBank}
%Knox, C. et al. Drugbank 3.0: a comprehensive resource for 'omics' research on drugs. Nucleic Acids Res. 39, D1035-D1041 (2011).
%
%\bibitem{ProteinDatabase}
%Prasad, T. S. K. et al. Human protein reference database-2009 update. Nucleic Acids Res. 37, D767-D772 (2009).
%
%\bibitem{ToxicogenomicsDatabase}
%Davis, A. P. et al. The comparative toxicogenomics database: update 2013. Nucleic Acids Res. 41, D1104-D1114 (2013).
%
%\bibitem{SideEffect}
%Kuhn, M., Campillos, M., Letunic, I., Jensen, L. J. and  Bork, P. A side effect resource to capture phenotypic effects of drugs. 6, 343 (2009).
\end{thebibliography}
%\balancecolumns
\end{document}
